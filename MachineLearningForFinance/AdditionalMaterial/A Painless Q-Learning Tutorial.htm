<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>
<title>A Painless Q-Learning Tutorial</title>
<meta name="description" content="Q-Learning. Step-By-Step Tutorial. This tutorial introduces the concept of Q-learning through a simple but comprehensive numerical example.  The example describes an agent which uses unsupervised training to learn about an unknown environment.  You might also find it helpful to compare this example with the accompanying source code examples. Suppose we have 5 rooms in a building connected by doors as shown in the figure below.  We\'ll number each room 0 through 4.  The outside of the building can be thought of as one big room (5).  Notice that doors 1 and 4 lead into the building from room 5 (outside). We can represent the rooms on a graph, each room as a node, and each door as a link. For this example, we\'d like to put an agent in any room, and from that room, go outside the building (this will be our target room). In other words, the goal room is number 5. To set this room as a goal, we\'ll associate a reward value to each door (i.e. link between nodes). The doors that lead immediately to the goal have an instant reward of 100.  Other doors not directly connected to the target room have zero reward. Because doors are two-way ( 0 leads to 4, and 4 leads back to 0 ), two arrows are assigned to each room. Each arrow contains an instant reward value, as shown below: Of course, Room 5 loops back to itself with a reward of 100, and all other direct connections to the goal room carry a reward of 100.  In Q-learning, the goal is to reach the state with the highest reward, so that if the agent arrives at the goal, it will remain there forever. This type of goal is called an \" absorbing="" goal\".="" imagine="" our="" agent="" as="" a="" dumb="" virtual="" robot="" that="" can="" learn="" through="" experience.="" the="" pass="" from="" one="" room="" to="" another="" but="" has="" no="" knowledge="" of="" environment,="" and="" doesn\'t="" know="" which="" sequence="" doors="" lead="" outside.="" suppose="" we="" want="" model="" some="" kind="" simple="" evacuation="" an="" any="" in="" building.="" now="" have="" 2="" reach="" outside="" house="" (5).="" terminology="" q-learning="" includes="" terms="" \"state\"="" \"action\".="" we\'ll="" call="" each="" room,="" including="" outside,="" \"state\",="" agent\'s="" movement="" will="" be="" diagram,="" is="" depicted="" node,="" while="" \"action\"="" represented="" by="" arrows.="" state="" 2.="" 2,="" it="" go="" 3="" because="" connected="" 3.="" however,="" cannot="" directly="" 1="" there="" direct="" door="" connecting="" (thus,="" arrows).="" 3,="" either="" or="" 4="" back="" (look="" at="" all="" arrows="" about="" 3).="" if="" 4,="" then="" three="" possible="" actions="" are="" 0,="" 5="" 1,="" only="" 4.="" put="" diagram="" instant="" reward="" values="" into="" following="" table,="" \"matrix="" r\".="" -1\'s="" table="" represent="" null="" (i.e.;="" where="" isn\'t="" link="" between="" nodes).="" for="" example,="" 0="" 1.="" add="" similar="" matrix,="" \"q\",="" brain="" agent,="" representing="" memory="" what="" learned="" rows="" matrix="" q="" current="" columns="" leading="" next="" (the="" links="" starts="" out="" knowing="" nothing,="" initialized="" zero.="" this="" simplicity="" explanation,="" assume="" number="" states="" known="" (to="" six).="" didn\'t="" how="" many="" were="" involved,="" could="" start="" with="" element.="" task="" more="" new="" found.="" transition="" rule="" learning="" very="" formula:="" q(state,="" action)="R(state," +="" gamma="" *="" max[q(next="" state,="" actions)].="" according="" formula,="" value="" assigned="" specific="" element="" q,="" equal="" sum="" corresponding="" r="" parameter="" gamma,="" multiplied="" maximum="" state.="" experience,="" without="" teacher="" (this="" called="" unsupervised="" learning).="" explore="" until="" reaches="" goal.="" exploration="" episode.="" episode="" consists="" moving="" initial="" goal="" time="" arrives="" program="" goes="" algorithm="" follows:="" set="" parameter,="" environment="" rewards="" r.="" initialize="" episode:="" select="" random="" do="" hasn\'t="" been="" reached.="" among="" using="" action,="" consider="" going="" get="" based="" on="" actions.="" compute:="" end="" do.="" for.="" above="" used="" equivalent="" training="" session.="" session,="" explores="" (represented="" ),="" receives="" (if="" any)="" purpose="" enhance="" \'brain\'="" q.="" results="" optimized="" case,="" enhanced,="" instead="" exploring="" around,="" forth="" same="" rooms,="" find="" fastest="" route="" range="" closer="" zero,="" tend="" immediate="" rewards.="" one,="" future="" greater="" weight,="" willing="" delay="" reward.="" use="" simply="" traces="" states,="" finds="" highest="" recorded="" state:="" utilize="" matrix:="" action="" value.="" repeat="" steps="" return="" example="" hand.="" understand="" works,="" few="" episodes="" step="" step.="" rest="" illustrated="" source="" code="" examples.="" setting="" zero="" look="" second="" row="" (state="" 1)="" two="" 1:="" 5.="" selection,="" action.="" let\'s="" would="" happen="" sixth="" (i.e.="" 5).="" actions:="" since="" still="" q(5,="" 1),="" 4),="" 5),="" result="" computation="" q(1,="" 5)="" 100="" r(5,="" 1).="" 5,="" becomes="" we\'ve="" finished="" contains="" updated="" as:="" episode,="" randomly="" chosen="" time,="" fourth="" r;="" then,="" compute="" value:="" last="" 3)="0" q(3,="" becomes:="" inner="" loop="" not="" so,="" starting="" lucky="" draw,="" selected="" now,="" imaging="" we\'re="" these="" entries="" does="" change="" matrix.="" finish="" contain="" learns="" further="" episodes,="" finally="" convergence="" like:="" normalized="" converted="" percentage)="" dividing="" non-zero="" (500="" case):="" once="" gets="" close="" enough="" convergence,="" most="" optimal="" paths="" tracing="" best="" sequences="" guide:="" suggests="" suggest="" alternatives:="" arbitrarily="" choose="" thus="" -="" 5."="">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link href="A%20Painless%20Q-Learning%20Tutorial_files/main.css" rel="stylesheet" type="text/css">
<link href="A%20Painless%20Q-Learning%20Tutorial_files/main_menu.css" rel="stylesheet" type="text/css">
<link href="A%20Painless%20Q-Learning%20Tutorial_files/left_menu.css" rel="stylesheet" type="text/css">
<link href="A%20Painless%20Q-Learning%20Tutorial_files/news_menu.css" rel="stylesheet" type="text/css">
<link href="A%20Painless%20Q-Learning%20Tutorial_files/notepage.css" rel="stylesheet" type="text/css">
</head>
<body data-pinterest-extension-installed="ff1.37.9">
	<a name="top"></a> <!-- for those "back to top" links -->

	<div class="main_container">
		<div class="header">
			<div class="control_row">
				<div class="crButton1"><a href="http://mnemstudio.org/contact.php">Contact ();</a></div>
				<div class="crButton2"><a href="http://mnemstudio.org/links-all-topics.htm">Links ();</a></div>
				<div class="search_controls">
					<input name="sitekeywords" class="search_box" type="text">
					<input name="searchSite" id="search_button" value="submit" src="A%20Painless%20Q-Learning%20Tutorial_files/search_button1.png" onclick="javascript: window.location='search.php?&amp;keywordBox=' + document.getElementsByName('sitekeywords').item(0).value + '&amp;strategy=all&amp;section=site';" type="image">
				</div>
			</div>

			<div id="menu1">
				<div class="menuButton1"><a href="http://mnemstudio.org/index.php">Home ();</a></div>
				<div class="menuButton1"><a href="http://mnemstudio.org/artificial-intelligence-introduction.htm">AI ();</a></div>
				<div class="menuButton1"><a href="http://mnemstudio.org/robots-introduction.htm">Robotics ();</a></div>
				<div class="menuButton2"><a href="http://mnemstudio.org/misc-notes-introduction.htm">Notes ();</a></div>
				<div class="menuButton2"><a href="http://mnemstudio.org/about.php">About ();</a></div>
			</div>
		</div><div class="navRow">
<div class="navBranch"><a href="http://mnemstudio.org/index.php">Home</a> &gt; <a href="http://mnemstudio.org/artificial-intelligence-introduction.htm">AI Main</a> &gt; <a href="http://mnemstudio.org/path-finding-introduction.htm">Path-Finding</a> &gt; <a href="http://mnemstudio.org/path-finding-q-learning.htm">Q-Learning</a> &gt; Tutorial</div>
<div class="socialBookmarks">
	<a href="http://delicious.com/save" onclick="window.open('http://delicious.com/save?v=5&amp;noui&amp;jump=close&amp;url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title), 'delicious','toolbar=no,width=550,height=550'); return false;"><img src="A%20Painless%20Q-Learning%20Tutorial_files/delicious16x16.gif" alt="Delicious" title="Bookmark this on delicious.com" width="16" height="16" border="0"></a>
	<img src="A%20Painless%20Q-Learning%20Tutorial_files/spacer_trans8x16.png">
	<script type="text/javascript">document.write('<a href="http://www.stumbleupon.com/submit?url='+encodeURIComponent(location.href)+'&title%3D'+encodeURIComponent(document.title)+'"><img src="images/16x16_su_solid.gif" alt="StumbleUpon.com" border="0" title="Share this with StumbleUpon.com" /></a>');</script><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fmnemstudio.org%2Fpath-finding-q-learning-tutorial.htm&amp;title%3DA%20Painless%20Q-Learning%20Tutorial"><img src="A%20Painless%20Q-Learning%20Tutorial_files/16x16_su_solid.gif" alt="StumbleUpon.com" title="Share this with StumbleUpon.com" border="0"></a>
	<img src="A%20Painless%20Q-Learning%20Tutorial_files/spacer_trans8x16.png">
	<script type="text/javascript">document.write('<a href="http://www.facebook.com/sharer.php?u='+encodeURIComponent(location.href)+'&t='+encodeURIComponent(document.title)+'"><img src="images/facebook16x16.gif" alt="Facebook.com" border="0" title="Share this on Facebook.com" /></a>');</script><a href="http://www.facebook.com/sharer.php?u=http%3A%2F%2Fmnemstudio.org%2Fpath-finding-q-learning-tutorial.htm&amp;t=A%20Painless%20Q-Learning%20Tutorial"><img src="A%20Painless%20Q-Learning%20Tutorial_files/facebook16x16.gif" alt="Facebook.com" title="Share this on Facebook.com" border="0"></a>
	<img src="A%20Painless%20Q-Learning%20Tutorial_files/spacer_trans8x16.png">
	<script type="text/javascript">document.write('<a href="http://digg.com/submit?url='+encodeURIComponent(location.href)+'&title='+encodeURIComponent(document.title)+'&bodytext=&media=news&topic=technology"><img src="images/digg_icon16x16.gif" alt="Digg.com" border="0" title="Digg this at Digg.com" /></a>');</script><a href="http://digg.com/submit?url=http%3A%2F%2Fmnemstudio.org%2Fpath-finding-q-learning-tutorial.htm&amp;title=A%20Painless%20Q-Learning%20Tutorial&amp;bodytext=&amp;media=news&amp;topic=technology"><img src="A%20Painless%20Q-Learning%20Tutorial_files/digg_icon16x16.gif" alt="Digg.com" title="Digg this at Digg.com" border="0"></a>
	
</div></div>
<div class="leftColumn">
<div class="leftMenuTitleDouble"><a href="http://mnemstudio.org/path-finding-introduction.htm">Path<br>Finding</a></div>
<div class="leftMenuItemSingle"><a href="http://mnemstudio.org/path-finding-q-learning.htm">Q-Learning</a></div>
<div class="leftLastMenuItemSingle"><a href="http://mnemstudio.org/path-finding-a-star.htm">A*</a></div><div class="leftMenuBottomSpacer"></div>
</div>
<div class="centerColumn">
<div class="topic_title"><div class="content_title"><h1>Q-Learning .*;</h1></div></div>

<div class="topic_passage">
<div class="content_header"><h1>Step-By-Step Tutorial</h1></div>
	<p>This tutorial introduces the concept of Q-learning through a simple 
but comprehensive numerical example.&nbsp; The example describes an 
agent which uses unsupervised training to learn about an unknown 
environment.&nbsp; You might also find it helpful to compare this 
example with the accompanying source code examples.</p>
	<p>Suppose we have 5 rooms in a building connected by doors as shown in
 the figure below.&nbsp; We'll number each room 0 through 4.&nbsp; The 
outside of the building can be thought of as one big room (5).&nbsp; 
Notice that doors 1 and 4 lead into the building from room 5 (outside).</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/modeling_environment_clip_image002a.gif" width="576" height="334" border="0"></p>
	<p>We can represent the rooms on a graph, each room as a node, and each door as a link.</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/map1a.gif" width="482" height="311" border="0"></p>
	<p>For this example, we'd like to put an agent in any room, and from 
that room, go outside the building (this will be our target room). In 
other words, the goal room is number 5. To set this room as a goal, 
we'll associate a reward value to each door (i.e. link between nodes). 
The doors that lead immediately to the goal have an instant reward of 
100.&nbsp; Other doors not directly connected to the target room have 
zero reward. Because doors are two-way ( 0 leads to 4, and 4 leads back 
to 0 ), two arrows are assigned to each room. Each arrow contains an 
instant 
reward value, as shown below:</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/map2a.gif" width="481" height="322" border="0"></p>
	<p>Of course, Room 5 loops back to itself with a reward of 100, and all
 other direct connections to the goal room carry a reward of 100.&nbsp; 
In Q-learning, the goal is to reach the state with the highest reward, 
so that if the agent arrives at the goal, it will remain there forever. 
This type of goal is called an "absorbing goal".</p>
	<p>Imagine our agent as a dumb virtual robot that can learn through 
experience. The agent can pass from one room to another but has no 
knowledge of the environment, and doesn't know which sequence of doors 
lead to the outside.</p>
	<p>Suppose we want to model some kind of simple evacuation of an agent 
from any room in the building. Now suppose we have an agent in Room 2 
and we want the agent to learn to reach outside the house (5).</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/agent_clip_image002.gif" width="576" height="334" border="0"></p>
	<p>The terminology in Q-Learning includes the terms "state" and "action".</p>
	<p>We'll call each room, including outside, a "state", and the agent's 
movement from one room to another will be an "action".&nbsp; In our 
diagram, a "state" is depicted as a node, while "action" is represented 
by the arrows.</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/map3a.gif" width="481" height="322" border="0"></p>
	<p>Suppose the agent is in state 2.&nbsp; From state 2, it can go to 
state 3 because state 2 is connected to 3.&nbsp; From state 2, however, 
the agent cannot directly go to state 1 because there is no direct door 
connecting room 1 and 2 (thus, no arrows).&nbsp; From state 3, it can go
 either to state 1 or 4 or back to 2 (look at all the arrows about state
 3).&nbsp; If the agent is in state 4, then the three possible actions 
are to go to state 0, 5 or 3.&nbsp; If the agent is in state 1, it can 
go either to state 5 or 3.&nbsp; From state 0, it can only go back to 
state 4.</p>
	<p>We can put the state diagram and the instant reward values into the following reward table, "matrix R".</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/r_matrix1.gif" width="304" height="202" border="0"></p>
	<blockquote>
		<p align="center">The -1's in the table represent null values (i.e.; 
where there isn't a link between nodes). For example, State 0 cannot go 
to State 1.</p>
	</blockquote>
	<p>Now we'll add a similar matrix, "Q", to the brain of our agent, 
representing the memory of what the agent has learned through 
experience.&nbsp; The rows of matrix Q represent the current state of 
the agent, and the columns represent the possible actions leading to the
 next state (the links between the nodes).</p>
	<p>The agent starts out knowing nothing, the matrix Q is initialized to
 zero.&nbsp; In this example, for the simplicity of explanation, we 
assume the number of states is known (to be six).&nbsp; If we didn't 
know how many states were involved, the matrix Q could start out with 
only one element.&nbsp; It is a simple task to add more columns and rows
 in matrix Q if a new state is found.</p>
	<p>The transition rule of Q learning is a very simple formula:</p>
	<blockquote>
		<p>Q(state, action) = R(state, action) + Gamma * Max<font size="4">[</font>Q(next state, all actions)<font size="4">]</font></p>
	</blockquote>
	<p>According to this formula, a value assigned to a specific element of
 matrix Q, is equal to the sum of the corresponding value in matrix R 
and the learning parameter Gamma, multiplied by the maximum value of Q 
for all possible actions in the next state.</p>
	<p>&nbsp;</p>
	<p>Our virtual agent will learn through experience, without a teacher 
(this is called unsupervised learning).&nbsp; The agent will explore 
from state to state until it reaches the goal. We'll call each 
exploration an episode.&nbsp; Each episode consists of the agent moving 
from the initial state to the goal state.&nbsp; Each time the agent 
arrives at the goal state, the program goes to the next episode.</p>
	<p>&nbsp;</p>
	<p>The Q-Learning algorithm goes as follows:</p>
	<blockquote>
		<p>1. Set the gamma parameter, and environment rewards in matrix R.</p>
		<p>2. Initialize matrix Q to zero.</p>
		<p>3. For each episode:</p>
		<blockquote>
			<p>Select a random initial state.</p>
			<p>Do While the goal state hasn't been reached.</p>
			<ul>
				<li>Select one among all possible actions for the current state.</li>
				<li>Using this possible action, consider going to the next state.</li>
				<li>Get maximum Q value for this next state based on all possible actions.</li>
				<li>Compute: Q(state, action) = R(state, action) + Gamma * Max<font size="4">[</font>Q(next state, all actions)<font size="4">]</font></li>
				<li>Set the next state as the current state.</li>
			</ul>
			<p>End Do</p>
		</blockquote>
		<p>End For</p>
	</blockquote>
	<p>The algorithm above is used by the agent to learn from 
experience.&nbsp; Each episode is equivalent to one training 
session.&nbsp; In each training session, the agent explores the 
environment (represented by matrix R ), receives the reward (if any) 
until it reaches the goal state. The purpose of the training is to 
enhance the 'brain' of our agent, represented by matrix Q.&nbsp; More 
training results in a more optimized matrix Q.&nbsp; In this case, if 
the matrix Q has been enhanced, instead of exploring around, and going 
back and forth to the same rooms, the agent will find the fastest route 
to the goal state.</p>
	<p>The Gamma parameter has a range of 0 to 1 (0 &lt;= Gamma &gt; 
1).&nbsp; If Gamma is closer to zero, the agent will tend to consider 
only immediate rewards.&nbsp; If Gamma is closer to one, the agent will 
consider future rewards with greater weight, willing to delay the 
reward.</p>
	<p>To use the matrix Q, the agent simply traces the sequence of states,
 from the initial state to goal state.&nbsp; The algorithm finds the 
actions with the highest reward values recorded in matrix Q for current 
state:</p>
	<p>Algorithm to utilize the Q matrix:</p>
	<blockquote>
		<p>1. Set current state = initial state.</p>
		<p>2. From current state, find the action 
		with the highest Q value.</p>
		<p>3. Set current state = next state.</p>
		<p>4. Repeat Steps 2 and 3 until current state = goal state.</p>
	</blockquote>
	<p>The algorithm above will return the sequence of states from the initial state to the goal state.</p>
	<p>&nbsp;</p>
	<div class="topic_passage_footer"></div>
</div>

<div class="topic_passage">
<div class="content_header"><h1>Q-Learning Example By Hand</h1></div>
	<p>To understand how the Q-learning algorithm works, we'll go through a
 few episodes step by step. The rest of the steps are illustrated in the
 source code examples.</p>
	<p>We'll start by setting the value of the learning parameter Gamma = 0.8, and the initial state as Room 1.</p>
	<p>Initialize matrix Q as a zero matrix:</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/q_matrix1.gif" width="227" height="177" border="0"></p>
	<p>Look at the second row (state 1) of matrix R.&nbsp; There are two 
possible actions for the current state 1: go to state 3, or go to state 
5. By random selection, we select to go to 5 as our action.</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/r_matrix1.gif" width="304" height="202" border="0"></p>
	<p>Now let's imagine what would happen if our agent were in state 
5.&nbsp; Look at the sixth row of the reward matrix R (i.e. state 
5).&nbsp; It has 3 possible actions: go to state 1, 4 or 5.</p>
	<blockquote>
		<p>Q(state, action) = R(state, action) + Gamma * Max<font size="4">[</font>Q(next state, all actions)<font size="4">]</font></p>
		<p>Q(1, 5) = R(1, 5) + 0.8 * Max<font size="4">[</font>Q(5, 1), Q(5, 4), Q(5, 5)<font size="4">] </font>= 100 + 0.8 * 0 = 100</p>
		<p>&nbsp;</p>
	</blockquote>
	<p>Since matrix Q is still initialized to zero, Q(5, 1), Q(5, 4), Q(5, 
5), are all zero.&nbsp; The result of this computation for Q(1, 5) is 
100 because of the instant reward from R(5, 1).</p>
	<p>The next state, 5, now becomes the current state.&nbsp; Because 5 is
 the goal state, we've finished one episode.&nbsp; Our agent's brain now
 contains an updated matrix Q as:</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/q_matrix2.gif" width="270" height="176" border="0"></p>
	<p>For the next episode, we start with a randomly chosen initial state.&nbsp; This time, we have state 3 as our initial state.</p>
	<p>Look at the fourth row of matrix R; it has 3 possible actions: go to
 state 1, 2 or 4.&nbsp; By random selection, we select to go to state 1 
as our action.</p>
	<p>Now we imagine that we are in state 1.&nbsp; Look at the second row 
of reward matrix R (i.e. state 1).&nbsp; It has 2 possible actions: go 
to state 3 or state 5.&nbsp; Then, we compute the Q value:</p>
	<blockquote>
		<p>Q(state, action) = R(state, action) + Gamma * Max<font size="4">[</font>Q(next state, all actions)<font size="4">]</font></p>
		<p>Q(1, 5) = R(1, 5) + 0.8 * Max<font size="4">[</font>Q(1, 2), Q(1, 5)<font size="4">] </font>= 0 + 0.8 * Max(0, 100) = 80</p>
	</blockquote>
	<p>We use the updated matrix Q from the last episode.&nbsp; Q(1, 3) = 0
 and Q(1, 5) = 100.&nbsp; The result of the computation is Q(3, 1) = 80 
because the reward is zero.&nbsp; The matrix Q becomes:</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/q_matrix3.gif" width="343" height="181" border="0"></p>
	<p>The next state, 1, now becomes the current state.&nbsp; We repeat 
the inner loop of the Q learning algorithm because state 1 is not the 
goal state.</p>
	<p>&nbsp;</p>
	<p>So, starting the new loop with the current state 1, there are two 
possible actions: go to state 3, or go to state 5.&nbsp; By lucky draw, 
our action selected is 5.</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/map3a.gif" width="481" height="322" border="0"></p>
	<p>Now, imaging we're in state 5, there are three possible actions: go 
to state 1, 4 or 5.&nbsp; We compute the Q value using the maximum value
 of these possible actions.</p>
	<blockquote>
		<p>Q(state, action) = R(state, action) + Gamma * Max<font size="4">[</font>Q(next state, all actions)<font size="4">]</font></p>
		<p>Q(1, 5) = R(1, 5) + 0.8 * Max<font size="4">[</font>Q(5, 1), Q(5, 4), Q(5, 5)<font size="4">] </font>= 100 + 0.8 * 0 = 100</p>
	</blockquote>
	<p>&nbsp;</p>
	<p>The updated entries of matrix Q, Q(5, 1), Q(5, 4), Q(5, 5), are all 
zero.&nbsp; The result of this computation for Q(1, 5) is 100 because of
 the instant reward from R(5, 1).&nbsp; This result does not change the Q
 matrix.</p>
	<p>Because 5 is the goal state, we finish this episode.&nbsp; Our agent's brain now contain updated matrix Q as:</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/q_matrix3.gif" width="343" height="181" border="0"></p>
	<p>If our agent learns more through further episodes, it will finally reach convergence values in matrix Q like:</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/q_matrix4.gif" width="347" height="175" border="0"></p>
	<p>This matrix Q, can then be normalized (i.e.; converted to 
percentage) by dividing all non-zero entries by the highest number (500 
in this case):</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/q_matrix5.gif" width="335" height="176" border="0"></p>
	<p>Once the matrix Q gets close enough to a state of convergence, we 
know our agent has learned the most optimal paths to the goal 
state.&nbsp; Tracing the best sequences of states is as simple as 
following the links with the highest values at each state.</p>
	<p align="center"><img src="A%20Painless%20Q-Learning%20Tutorial_files/map5.gif" width="483" height="319" border="0"></p>
	<p>For example, from initial State 2, the agent can use the matrix Q as a guide:</p>
	<blockquote>
		<p>From State 2 the maximum Q values suggests the action to go to state 3.</p>
		<p>From State 3 the maximum Q values suggest two alternatives: go to 
state 1 or 4.&nbsp; Suppose we arbitrarily choose&nbsp; to go to 1.</p>
		<p>From State 1 the maximum Q values suggests the action to go to state 5.</p>
		<p>Thus the sequence is 2 - 3 - 1 - 5.</p>
	</blockquote>
	
	<p>&nbsp;</p>
	<div class="topic_passage_footer"></div>
</div>



</div>
<div class="rightColumn">
<div class="newsMenu">
<div class="newsMenuTitle">News Links:</div>
<div class="newsMenuTopSpacer"></div>
<div class="rightMenuYearItem"><a href="http://mnemstudio.org/news-2017.htm">2017 (0)</a></div>
<div class="rightMenuMonthItem"><a href="http://mnemstudio.org/news-2017-06.htm">June (0)</a></div>
<div class="rightMenuMonthItem"><a href="http://mnemstudio.org/news-2017-05.htm">May (0)</a></div>
<div class="rightMenuMonthItem"><a href="http://mnemstudio.org/news-2017-04.htm">April (0)</a></div>
<div class="rightMenuMonthItem"><a href="http://mnemstudio.org/news-2017-03.htm">March (0)</a></div>
<div class="rightMenuMonthItem"><a href="http://mnemstudio.org/news-2017-02.htm">February (0)</a></div>
<div class="rightMenuMonthItem"><a href="http://mnemstudio.org/news-2017-01.htm">January (0)</a></div>
<div class="rightMenuYearItem"><a href="http://mnemstudio.org/news-2016.htm">2016 (0)</a></div>
<div class="rightMenuYearItem"><a href="http://mnemstudio.org/news-2015.htm">2015 (0)</a></div>
<div class="rightMenuYearItem"><a href="http://mnemstudio.org/news-2014.htm">2014 (0)</a></div>
<div class="rightMenuYearItem"><a href="http://mnemstudio.org/news-2013.htm">2013 (32)</a></div>
<div class="rightMenuYearItem"><a href="http://mnemstudio.org/news-2012.htm">2012 (242)</a></div>
<div class="rightMenuYearItem"><a href="http://mnemstudio.org/news-2011.htm">2011 (217)</a></div>
<div class="rightMenuYearItem"><a href="http://mnemstudio.org/news-2010.htm">2010 (185)</a></div>
<div class="rightMenuYearItem"><a href="http://mnemstudio.org/news-2009.htm">2009 (20)</a></div>
<div class="newsMenuBottomSpacer"></div>
</div>
		<div class="searchNewsLinks">
			Search News Links:
			<input name="searchNewsBox" id="searchNewsBox" type="text">
			<input name="searchNewsButton" id="searchNewsButton" value="Search News" onclick="javascript: window.location='search.php?&amp;keywordBox=' + document.getElementsByName('searchNewsBox').item(0).value + '&amp;strategy=all&amp;section=news';" type="button">
		</div></div>
</div>
<div class="mainFooter">
	<div class="mainFooterStart">public void footer() {</div>
	<div class="mainFooterContent">
		<div class="mainFooterRow1"><a href="http://mnemstudio.org/about.php">About</a> | <a href="http://mnemstudio.org/contact.php">Contact</a> | Privacy Policy | <a href="http://mnemstudio.org/terms-of-service.php">Terms of Service</a> | <a href="http://mnemstudio.org/sitemap.php">Site Map</a></div>
		<div class="mainFooterRow2">Copyright© 2009-2012 John McCullock. All Rights Reserved.</div>
	</div>
	<div class="mainFooterEnd">}</div>
</div>
<span style="border-radius: 2px; text-indent: 20px; width: auto; padding: 0px 4px 0px 0px; text-align: center; font: bold 11px/20px &quot;Helvetica Neue&quot;,Helvetica,sans-serif; color: rgb(255, 255, 255); background: rgb(189, 8, 28) url(&quot;data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMzBweCIgd2lkdGg9IjMwcHgiIHZpZXdCb3g9Ii0xIC0xIDMxIDMxIj48Zz48cGF0aCBkPSJNMjkuNDQ5LDE0LjY2MiBDMjkuNDQ5LDIyLjcyMiAyMi44NjgsMjkuMjU2IDE0Ljc1LDI5LjI1NiBDNi42MzIsMjkuMjU2IDAuMDUxLDIyLjcyMiAwLjA1MSwxNC42NjIgQzAuMDUxLDYuNjAxIDYuNjMyLDAuMDY3IDE0Ljc1LDAuMDY3IEMyMi44NjgsMC4wNjcgMjkuNDQ5LDYuNjAxIDI5LjQ0OSwxNC42NjIiIGZpbGw9IiNmZmYiIHN0cm9rZT0iI2ZmZiIgc3Ryb2tlLXdpZHRoPSIxIj48L3BhdGg+PHBhdGggZD0iTTE0LjczMywxLjY4NiBDNy41MTYsMS42ODYgMS42NjUsNy40OTUgMS42NjUsMTQuNjYyIEMxLjY2NSwyMC4xNTkgNS4xMDksMjQuODU0IDkuOTcsMjYuNzQ0IEM5Ljg1NiwyNS43MTggOS43NTMsMjQuMTQzIDEwLjAxNiwyMy4wMjIgQzEwLjI1MywyMi4wMSAxMS41NDgsMTYuNTcyIDExLjU0OCwxNi41NzIgQzExLjU0OCwxNi41NzIgMTEuMTU3LDE1Ljc5NSAxMS4xNTcsMTQuNjQ2IEMxMS4xNTcsMTIuODQyIDEyLjIxMSwxMS40OTUgMTMuNTIyLDExLjQ5NSBDMTQuNjM3LDExLjQ5NSAxNS4xNzUsMTIuMzI2IDE1LjE3NSwxMy4zMjMgQzE1LjE3NSwxNC40MzYgMTQuNDYyLDE2LjEgMTQuMDkzLDE3LjY0MyBDMTMuNzg1LDE4LjkzNSAxNC43NDUsMTkuOTg4IDE2LjAyOCwxOS45ODggQzE4LjM1MSwxOS45ODggMjAuMTM2LDE3LjU1NiAyMC4xMzYsMTQuMDQ2IEMyMC4xMzYsMTAuOTM5IDE3Ljg4OCw4Ljc2NyAxNC42NzgsOC43NjcgQzEwLjk1OSw4Ljc2NyA4Ljc3NywxMS41MzYgOC43NzcsMTQuMzk4IEM4Ljc3NywxNS41MTMgOS4yMSwxNi43MDkgOS43NDksMTcuMzU5IEM5Ljg1NiwxNy40ODggOS44NzIsMTcuNiA5Ljg0LDE3LjczMSBDOS43NDEsMTguMTQxIDkuNTIsMTkuMDIzIDkuNDc3LDE5LjIwMyBDOS40MiwxOS40NCA5LjI4OCwxOS40OTEgOS4wNCwxOS4zNzYgQzcuNDA4LDE4LjYyMiA2LjM4NywxNi4yNTIgNi4zODcsMTQuMzQ5IEM2LjM4NywxMC4yNTYgOS4zODMsNi40OTcgMTUuMDIyLDYuNDk3IEMxOS41NTUsNi40OTcgMjMuMDc4LDkuNzA1IDIzLjA3OCwxMy45OTEgQzIzLjA3OCwxOC40NjMgMjAuMjM5LDIyLjA2MiAxNi4yOTcsMjIuMDYyIEMxNC45NzMsMjIuMDYyIDEzLjcyOCwyMS4zNzkgMTMuMzAyLDIwLjU3MiBDMTMuMzAyLDIwLjU3MiAxMi42NDcsMjMuMDUgMTIuNDg4LDIzLjY1NyBDMTIuMTkzLDI0Ljc4NCAxMS4zOTYsMjYuMTk2IDEwLjg2MywyNy4wNTggQzEyLjA4NiwyNy40MzQgMTMuMzg2LDI3LjYzNyAxNC43MzMsMjcuNjM3IEMyMS45NSwyNy42MzcgMjcuODAxLDIxLjgyOCAyNy44MDEsMTQuNjYyIEMyNy44MDEsNy40OTUgMjEuOTUsMS42ODYgMTQuNzMzLDEuNjg2IiBmaWxsPSIjYmQwODFjIj48L3BhdGg+PC9nPjwvc3ZnPg==&quot;) no-repeat scroll 3px 50% / 14px 14px; position: absolute; opacity: 1; z-index: 8675309; display: block; cursor: pointer; border: medium none; top: 450px; left: 684px;">Save</span></body></html>